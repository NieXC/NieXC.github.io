<head>
    <title>ProjectName</title>
    <!-- Meta -->
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="Yunchao Wei's homepage">
    <link rel="shortcut icon" href="../images/log.png">

    <link href='https://fonts.googleapis.com/css?family=Roboto:400,500,400italic,300italic,300,500italic,700,700italic,900,900italic' rel='stylesheet' type='text/css'>
    <!-- Global CSS -->
    <link rel="stylesheet" href="../css/bootstrap.min.css">
	<link rel="stylesheet" href="../css/bootstrap-responsive.min.css">
	<link rel="stylesheet" href="../css/font-awesome/css/font-awesome.min.css">

    <link rel="stylesheet" href="../css/main.css">

    <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
      <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->

</head>
<body>
<div class="container">
       <div class="projectheader">
			<h2>Cross-Modal Retrieval With CNN Visual Features: A New Baseline</h2>

       </div>

		<h4> Introduction </h4>
		<p>
			Recently, convolutional neural network (CNN) visual features have demonstrated their powerful ability as a universal representation for various recognition tasks. In this paper, cross-modal retrieval with CNN visual features is implemented with several classic methods. Specifically, off-the-shelf CNN visual features are extracted from the CNN model, which is pretrained on ImageNet with more than one million images from 1000 object categories, as a generic image representation to tackle cross-modal retrieval. To further enhance the representational ability of CNN visual features, based on the pretrained CNN model on ImageNet, a fine-tuning step is performed by using the open source Caffe CNN library for each target data set. Besides, we propose a deep semantic matching method to address the cross-modal retrieval problem with respect to samples which are annotated with one or multiple labels. Extensive experiments on five popular publicly available data sets well demonstrate the superiority of CNN visual features for cross-modal retrieval.
		</p>


		<div class="projectimg">
			<img alt="" src="../images/tcyb.png"></td>
		</div>

		<h4>Downloads</h4>

			<p>The feature data can be downloaded from <a href="https://drive.google.com/open?id=0B5AHi8MAbKMLclBFSmtIWnJBSjQ"> <b>here</b></a>.</p>



</div>
</body>

</html>
